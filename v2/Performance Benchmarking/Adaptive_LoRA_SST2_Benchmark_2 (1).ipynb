{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOuyRKxEfLGKSDM0sp8pTqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"543a301f62364d73a44e5f6e4df5987c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bcc936f51644d8298cfd1d9e90ac927","IPY_MODEL_9662334189664547826c5d181d497460","IPY_MODEL_31831c1e03a643d386009b329471c389"],"layout":"IPY_MODEL_0e34ff0069044acaafd14ac92786d8d8"}},"9bcc936f51644d8298cfd1d9e90ac927":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6c756fc198b4fa7ae4656954b28239a","placeholder":"â€‹","style":"IPY_MODEL_679fd4417d7b42aeb5632d31918ce538","value":"Map:â€‡100%"}},"9662334189664547826c5d181d497460":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfc7e58c0c4f45458f77777556beafd7","max":872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed2cb618f80f437f8e065babc197a5e7","value":872}},"31831c1e03a643d386009b329471c389":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09a69fd9726a401cb27fbb8271587e00","placeholder":"â€‹","style":"IPY_MODEL_3b31dbc469474dfc8a460a2c3af3bb5c","value":"â€‡872/872â€‡[00:00&lt;00:00,â€‡12382.25â€‡examples/s]"}},"0e34ff0069044acaafd14ac92786d8d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6c756fc198b4fa7ae4656954b28239a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"679fd4417d7b42aeb5632d31918ce538":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfc7e58c0c4f45458f77777556beafd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed2cb618f80f437f8e065babc197a5e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09a69fd9726a401cb27fbb8271587e00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b31dbc469474dfc8a460a2c3af3bb5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s8MlbCKp-4kO","executionInfo":{"status":"ok","timestamp":1745771363693,"user_tz":420,"elapsed":72398,"user":{"displayName":"Aditya Dawadikar","userId":"04078820344684524161"}},"outputId":"6411fd41-5138-40a9-fe89-8c6ef0512de9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q torch transformers datasets evaluate scikit-learn pandas matplotlib"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import random\n","from sklearn.metrics import accuracy_score\n","from transformers import (\n","    DistilBertTokenizerFast, DistilBertForSequenceClassification,\n","    Trainer, TrainingArguments, TrainerCallback\n",")\n","from datasets import load_dataset\n","import evaluate\n","import gc"],"metadata":{"id":"cvhQgl9h_Bjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set Seed for reproducability\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","set_seed(42)"],"metadata":{"id":"2EuLwirb_v6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AdaptiveLoRALinear(nn.Module):\n","    def __init__(self, base_layer, r=4, alpha=32, dropout=0.05):\n","        super().__init__()\n","        self.base = base_layer\n","        self.r = r\n","        self.alpha = alpha\n","        self.scaling = alpha / r\n","        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n","\n","        self.lora_A = nn.Parameter(torch.randn(r, base_layer.in_features) * 0.01)\n","        self.lora_B = nn.Parameter(torch.randn(base_layer.out_features, r) * 0.01)\n","\n","        device = self.lora_A.device\n","        self.initial_A = self.lora_A.detach().clone().to(device)\n","        self.initial_B = self.lora_B.detach().clone().to(device)\n","\n","        self.grad_norm_history = []\n","        self.weight_change_history = []\n","\n","        self.lora_A.register_hook(self._capture_grad_hook('A'))\n","        self.lora_B.register_hook(self._capture_grad_hook('B'))\n","\n","    def forward(self, x):\n","        result = self.base(x)\n","        if self.r > 0:\n","            lora_out = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n","            result += self.scaling * lora_out\n","        return result\n","\n","    def _capture_grad_hook(self, which):\n","        def hook(grad):\n","            norm = grad.norm().item()\n","            if which == 'A':\n","                self.grad_norm_history.append(('A', norm))\n","            elif which == 'B':\n","                self.grad_norm_history.append(('B', norm))\n","        return hook\n","\n","    def compute_weight_change(self):\n","        device = self.lora_A.device\n","        if self.initial_A.device != device:\n","            self.initial_A = self.initial_A.to(device)\n","        if self.initial_B.device != device:\n","            self.initial_B = self.initial_B.to(device)\n","        delta_A = (self.lora_A - self.initial_A).norm().item()\n","        delta_B = (self.lora_B - self.initial_B).norm().item()\n","        total_change = delta_A + delta_B\n","        self.weight_change_history.append(total_change)\n","        return total_change\n","\n","    def average_grad_norm(self):\n","        norms = [n for (w, n) in self.grad_norm_history]\n","        return sum(norms) / len(norms) if norms else 0.0\n","\n","\n","class AdaptiveLoRAMonitor:\n","    def __init__(self, model, alpha=0.5, beta=0.5, min_r=2, max_r=16):\n","        self.model = model\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.min_r = min_r\n","        self.max_r = max_r\n","        self.modules = [m for m in model.modules() if isinstance(m, AdaptiveLoRALinear)]\n","\n","    def assign_adaptive_ranks(self, run_id):\n","        stats = []\n","        for module in self.modules:\n","            grad_score = module.average_grad_norm()\n","            weight_score = module.compute_weight_change()\n","            total_score = self.alpha * grad_score + self.beta * weight_score\n","            stats.append((module, grad_score, weight_score, total_score))\n","\n","        scores = [s[-1] for s in stats]\n","        valid_scores = [s for s in scores if np.isfinite(s)]\n","\n","\n","        # Clean NaN/inf from scores and set fallback normalization\n","        # scores = [s if np.isfinite(s) else 0.0 for s in scores]\n","\n","        # scores_clean = [s if np.isfinite(s) else None for s in scores]\n","        # valid_scores = [s for s in scores_clean if s is not None]\n","\n","        if not valid_scores:\n","            raise ValueError(\"All scores are NaN or Inf â€” cannot assign ranks!\")\n","\n","        min_score, max_score = min(valid_scores), max(valid_scores)\n","\n","        fallback_rank = (self.min_r+self.max_r) // 2\n","\n","        print(\"\\nðŸ“Š Layer-wise Stats and Rank Assignment:\")\n","        print(f\"{'Layer':<30} {'Grad Norm':>12} {'Weight Î”':>12} {'Score':>12} {'Norm Score':>12} {'Assigned r':>10}\")\n","        rows = []\n","\n","        for (module, grad, weight, score) in stats:\n","            if not np.isfinite(score):\n","                # Exploded layer => directly assign mid rank\n","                new_r = fallback_rank\n","                norm_score = 0.5  # Just for logging/printing\n","            else:\n","                if min_score == max_score:\n","                    norm_score = 1.0\n","                else:\n","                    norm_score = (score - min_score) / (max_score - min_score)\n","                new_r = int(self.min_r + (self.max_r - self.min_r) * norm_score)\n","\n","            new_r = max(self.min_r, min(self.max_r, new_r))  # Clamp safely\n","\n","            # Reinitialize LoRA matrices\n","            device = module.lora_A.device\n","            module.lora_A = nn.Parameter(torch.randn(new_r, module.base.in_features, device=device) * 0.01)\n","            module.lora_B = nn.Parameter(torch.randn(module.base.out_features, new_r, device=device) * 0.01)\n","            module.initial_A = module.lora_A.detach().clone().to(device)\n","            module.initial_B = module.lora_B.detach().clone().to(device)\n","            module.r = new_r\n","            module.grad_norm_history = []\n","            module.weight_change_history = []\n","\n","            print(f\"{str(module.base)[:28]:<30} {grad:12.4f} {weight:12.4f} {score:12.4f} {norm_score:12.4f} {new_r:10d}\")\n","\n","            rows.append({\n","                \"Layer\": str(module.base),\n","                \"Grad Norm\": grad,\n","                \"Weight Change\": weight,\n","                \"Score\": score,\n","                \"Norm Score\": norm_score,\n","                \"Assigned Rank\": new_r,\n","            })\n","\n","        # Save to CSV\n","        df_layer_stats = pd.DataFrame(rows)\n","        os.makedirs(\"rank_logs\", exist_ok=True)\n","        df_layer_stats.to_csv(f\"rank_logs/adaptive_lora_run_{run_id}.csv\", index=False)\n","\n","        # normalized_scores = []\n","        # fallback_rank = int((self.min_r + self.max_r) / 2)\n","        # fallback_norm_score = (fallback_rank - self.min_r) / (self.max_r - self.min_r)\n","\n","        # for s in scores_clean:\n","        #     if s is None:\n","        #         normalized_scores.append(fallback_norm_score)\n","        #     else:\n","        #         if min_score == max_score:\n","        #             norm_s = 1.0  # Edge case: all scores same\n","        #         else:\n","        #             norm_s = (s - min_score) / (max_score - min_score)\n","        #         normalized_scores.append(norm_s)\n","\n","        # print(\"\\nLayer-wise Stats and Rank Assignment:\")\n","        # print(f\"{'Layer':<30} {'Grad Norm':>12} {'Weight Î”':>12} {'Score':>12} {'Norm Score':>12} {'Assigned r':>10}\")\n","        # rows = []\n","        # for (module, g, w, score), norm_score in zip(stats, normalized_scores):\n","        #     new_r = int(self.min_r + (self.max_r - self.min_r) * norm_score)\n","        #     new_r = max(self.min_r, min(self.max_r, new_r))\n","\n","        #     device = module.lora_A.device\n","        #     module.lora_A = nn.Parameter(torch.randn(new_r, module.base.in_features, device=device) * 0.01)\n","        #     module.lora_B = nn.Parameter(torch.randn(module.base.out_features, new_r, device=device) * 0.01)\n","        #     module.initial_A = module.lora_A.detach().clone().to(device)\n","        #     module.initial_B = module.lora_B.detach().clone().to(device)\n","        #     module.r = new_r\n","        #     module.grad_norm_history = []\n","        #     module.weight_change_history = []\n","\n","        #     print(f\"{str(module.base)[:28]:<30} {g:12.4f} {w:12.4f} {score:12.4f} {norm_score:12.4f} {new_r:10d}\")\n","\n","        #     rows.append({\n","        #         \"Layer\": str(module.base),\n","        #         \"Grad Norm\": g,\n","        #         \"Weight Change\": w,\n","        #         \"Score\": score,\n","        #         \"Norm Score\": norm_score,\n","        #         \"Assigned Rank\": new_r,\n","        #     })\n","\n","        # df_layer_stats = pd.DataFrame(rows)\n","        # os.makedirs(\"rank_logs\", exist_ok=True)\n","        # df_layer_stats.to_csv(f\"rank_logs/adaptive_lora_run_{run_id}.csv\", index=False)\n"],"metadata":{"id":"oOZ-vC9D_y-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inject_adaptive_lora(model, r=4, alpha=32, dropout=0.05):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear) and ('q_lin' in name or 'v_lin' in name):\n","            parent = model\n","            for part in name.split('.')[:-1]:\n","                parent = getattr(parent, part)\n","            layer_name = name.split('.')[-1]\n","            setattr(parent, layer_name, AdaptiveLoRALinear(module, r, alpha, dropout))\n","    return model\n","\n","def freeze_model_except_lora(model):\n","    \"\"\"\n","      Freezes all the layers except the LoRA Layers and Classification Layers\n","    \"\"\"\n","    for name, param in model.named_parameters():\n","        if (\n","            'lora_A' in name or\n","            'lora_B' in name or\n","            'pre_classifier' in name or\n","            'classifier' in name\n","        ):\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = False"],"metadata":{"id":"Y1fqmnBkADKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","dataset = load_dataset(\"glue\", \"sst2\")\n","dataset = dataset.rename_column(\"label\", \"labels\")\n","dataset = dataset.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\n","dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"],"metadata":{"id":"kpCy39B1AGK1","executionInfo":{"status":"ok","timestamp":1745771858981,"user_tz":420,"elapsed":3175,"user":{"displayName":"Aditya Dawadikar","userId":"04078820344684524161"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["543a301f62364d73a44e5f6e4df5987c","9bcc936f51644d8298cfd1d9e90ac927","9662334189664547826c5d181d497460","31831c1e03a643d386009b329471c389","0e34ff0069044acaafd14ac92786d8d8","c6c756fc198b4fa7ae4656954b28239a","679fd4417d7b42aeb5632d31918ce538","cfc7e58c0c4f45458f77777556beafd7","ed2cb618f80f437f8e065babc197a5e7","09a69fd9726a401cb27fbb8271587e00","3b31dbc469474dfc8a460a2c3af3bb5c"]},"outputId":"616080c7-9149-4044-e185-060a2e10f804"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/872 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543a301f62364d73a44e5f6e4df5987c"}},"metadata":{}}]},{"cell_type":"code","source":["os.makedirs(\"rank_logs\", exist_ok=True)"],"metadata":{"id":"T0ExdVPYAxJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"iPdrEwzAKNzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def log_phase(phase_name, run_id=None):\n","    ts = time.time()\n","    with open(\"gpu_phase_timestamps.log\", \"a\") as f:\n","        f.write(f\"{run_id},{phase_name},{ts}\\n\")"],"metadata":{"id":"pc1JdjfepUeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["warmup_steps = 500\n","alpha = 0.2\n","beta = 0.8"],"metadata":{"id":"Vvjcpwxr9bwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_results = []\n","\n","for run_id in range(6):\n","    print(f\"\\n\\nStarting Run {run_id} | Warmup Steps = {warmup_steps}\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    set_seed(42+run_id)\n","\n","    # initialize base model\n","    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","\n","    # Initial LoRA rank for Warmup phase\n","    # alpha must be either 2x or 4x of the rank r\n","    # if r = 4, then 4x4 = 16, thus alpha=16\n","    model = inject_adaptive_lora(model,\n","                                 r=4,\n","                                 alpha=16,\n","                                 dropout=0.05)\n","\n","    # freeze the model\n","    #   except the LoRA Layer\n","    #   except the Classification Heads\n","    freeze_model_except_lora(model)\n","\n","    # move model to CUDA\n","    model = model.to(\"cuda\")\n","    for name, param in model.named_parameters():\n","        if 'lora_' in name:\n","            param.data = param.data.to('cuda')\n","\n","    # Assign Adaptive Ranks\n","    monitor = AdaptiveLoRAMonitor(model,\n","                                  alpha=alpha,\n","                                  beta=beta,\n","                                  min_r=2,\n","                                  max_r=16)\n","\n","    # training arguments for warmup phase\n","    trainer = Trainer(\n","        model=model,\n","        args=TrainingArguments(\n","            output_dir=f\"./warmup_run{run_id}\",\n","            eval_strategy=\"no\",\n","            max_steps=warmup_steps,\n","            per_device_train_batch_size=16,\n","            learning_rate=5e-4,\n","            weight_decay=0.01,\n","            logging_steps=50,\n","            report_to=\"none\",\n","            fp16=True\n","        ),\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"validation\"]\n","    )\n","\n","    #------------------Warmup START------------------\n","    log_phase(\"warmup_start\", run_id)\n","    warmup_start = time.time()\n","    trainer.train()\n","    warmup_end = time.time()\n","    log_phase(\"warmup_end\", run_id)\n","    #------------------Warmup END------------------\n","\n","    #------------------Rank Assignment START------------------\n","    log_phase(\"rank_assignment_start\", run_id)\n","    monitor.assign_adaptive_ranks(run_id=run_id)\n","    log_phase(\"rank_assignment_end\", run_id)\n","    #------------------Rank Assignment END------------------\n","\n","    # Training arguments for Fine Tuning\n","    trainer = Trainer(\n","        model=model,\n","        args=TrainingArguments(\n","            output_dir=f\"./finetune_run{run_id}\",\n","            eval_strategy=\"epoch\",\n","            save_strategy=\"no\",\n","            num_train_epochs=3,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            learning_rate=5e-4,\n","            weight_decay=0.01,\n","            logging_steps=50,\n","            report_to=\"none\",\n","            fp16=True\n","        ),\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"validation\"],\n","        compute_metrics=compute_metrics\n","    )\n","\n","\n","    #------------------Training Phase START------------------\n","    log_phase(\"training_start\", run_id)\n","    start_time = time.time()\n","    trainer.train()\n","    end_time = time.time()\n","    log_phase(\"training_end\", run_id)\n","    #------------------Training Phase END------------------\n","\n","\n","    # final_memory = torch.cuda.memory_allocated() / 1e9\n","    outputs = trainer.predict(dataset[\"validation\"])\n","    logits = outputs.predictions[1] if isinstance(outputs.predictions, tuple) else outputs.predictions\n","    preds = np.argmax(logits, axis=-1)\n","    labels = dataset[\"validation\"][\"labels\"]\n","    acc = accuracy_score(labels.cpu().numpy(), preds)\n","\n","    print(f\"\\nFinal Eval Accuracy: {acc:.4f}\")\n","\n","    warmup_time = round((warmup_end - warmup_start)/ 60, 2)\n","    training_time = round((end_time - start_time) / 60, 2)\n","    total_time = round(warmup_time + training_time,2)\n","\n","    all_results.append({\n","        \"Run ID\": run_id,\n","        \"Warmup Steps\": warmup_steps,\n","        \"Final Accuracy\": round(acc, 4),\n","        \"Warmup Time (min)\": warmup_time,\n","        \"Training Time (min)\": training_time,\n","        \"Total Time (min)\": total_time\n","    })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"06C-O-FpAI0V","executionInfo":{"status":"ok","timestamp":1745774585067,"user_tz":420,"elapsed":2589385,"user":{"displayName":"Aditya Dawadikar","userId":"04078820344684524161"}},"outputId":"50c801c3-07f8-4497-b7ee-12b6ee71a317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Starting Run 0 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:18, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.508700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.361300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.314900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.363500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.292200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.322400</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.340400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.322300</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.302400</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.317300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      2097.5599       1.0462    1678.2571       0.0071          2\n","Linear(in_features=768, out_      8337.5685       0.9748    6670.2497       0.4974          8\n","Linear(in_features=768, out_      2006.9954       1.0933    1605.8150       0.0000          2\n","Linear(in_features=768, out_     12401.7395       0.9236    9921.5763       0.8168         13\n","Linear(in_features=768, out_      2079.2703       0.9106    1663.5984       0.0057          2\n","Linear(in_features=768, out_            inf       0.8747          inf       0.5000          9\n","Linear(in_features=768, out_      4232.2678       1.2049    3386.0552       0.1749          4\n","Linear(in_features=768, out_            inf       0.9665          inf       0.5000          9\n","Linear(in_features=768, out_      7134.8524       1.2844    5708.1388       0.4029          7\n","Linear(in_features=768, out_     14733.8770       1.0039   11787.3024       1.0000         16\n","Linear(in_features=768, out_      5199.6591       1.5043    4160.0282       0.2509          5\n","Linear(in_features=768, out_            inf       0.8919          inf       0.5000          9\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:51, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.233900</td>\n","      <td>0.260288</td>\n","      <td>0.897936</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.148900</td>\n","      <td>0.331716</td>\n","      <td>0.894495</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.158800</td>\n","      <td>0.359134</td>\n","      <td>0.909404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9094\n","\n","\n","Starting Run 1 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:17, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.519300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.362600</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.324800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.353600</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.296500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.315200</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.336500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.322200</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.303100</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.316600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      1857.4861       1.0115    1486.1912       0.0000          2\n","Linear(in_features=768, out_      7488.8164       1.0017    5991.2535       0.5855         10\n","Linear(in_features=768, out_      1950.6023       1.0508    1560.6920       0.0097          2\n","Linear(in_features=768, out_      9842.5633       0.9128    7874.2332       0.8302         13\n","Linear(in_features=768, out_      1970.9588       0.9645    1576.9600       0.0118          2\n","Linear(in_features=768, out_            inf       0.8670          inf       0.5000          9\n","Linear(in_features=768, out_      4359.9941       1.2289    3488.2410       0.2602          5\n","Linear(in_features=768, out_            inf       0.9939          inf       0.5000          9\n","Linear(in_features=768, out_      6954.5780       1.3370    5563.9298       0.5300          9\n","Linear(in_features=768, out_     11475.5387       0.9645    9180.6238       1.0000         16\n","Linear(in_features=768, out_      5604.0596       1.5623    4483.5601       0.3896          7\n","Linear(in_features=768, out_            inf       0.9186          inf       0.5000          9\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:36, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.248600</td>\n","      <td>0.254042</td>\n","      <td>0.899083</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.162500</td>\n","      <td>0.317888</td>\n","      <td>0.895642</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.164200</td>\n","      <td>0.332196</td>\n","      <td>0.905963</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9060\n","\n","\n","Starting Run 2 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:17, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.498000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.366200</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.316700</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.354400</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.295000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.318400</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.338800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.313500</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.300500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.319600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      1539.7719       1.0019    1232.0179       0.0000          2\n","Linear(in_features=768, out_      7547.5324       1.0008    6038.2261       0.6015         10\n","Linear(in_features=768, out_      1667.3364       1.0729    1334.0837       0.0128          2\n","Linear(in_features=768, out_     11277.2925       0.9403    9022.0221       0.9749         15\n","Linear(in_features=768, out_      1917.0634       0.9505    1533.8409       0.0378          2\n","Linear(in_features=768, out_     11528.0763       0.8774    9222.6365       1.0000         16\n","Linear(in_features=768, out_      3648.6241       1.2533    2919.1499       0.2111          4\n","Linear(in_features=768, out_            inf       0.9942          inf       0.5000          9\n","Linear(in_features=768, out_      6519.6592       1.3501    5215.9974       0.4986          8\n","Linear(in_features=768, out_     11093.8814       1.0035    8875.3058       0.9565         15\n","Linear(in_features=768, out_      4599.3602       1.4722    3679.7826       0.3063          6\n","Linear(in_features=768, out_            inf       0.8671          inf       0.5000          9\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:53, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.247100</td>\n","      <td>0.284899</td>\n","      <td>0.893349</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.128100</td>\n","      <td>0.340022</td>\n","      <td>0.904817</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.152500</td>\n","      <td>0.363111</td>\n","      <td>0.903670</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9037\n","\n","\n","Starting Run 3 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:17, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.500100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.318300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.353900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.294700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.311600</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.333700</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.321200</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.299500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.315200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      1894.7279       0.9504    1515.9724       0.0000          2\n","Linear(in_features=768, out_      9102.4437       1.0893    7282.1728       0.4990          8\n","Linear(in_features=768, out_      2341.7030       1.0842    1873.5793       0.0309          2\n","Linear(in_features=768, out_     13331.4228       0.8996   10665.3182       0.7917         13\n","Linear(in_features=768, out_      2447.1973       0.9809    1957.9540       0.0382          2\n","Linear(in_features=768, out_     16339.7775       0.8407   13071.9901       1.0000         16\n","Linear(in_features=768, out_      4721.6390       1.2269    3777.5565       0.1957          4\n","Linear(in_features=768, out_            inf       0.9839          inf       0.5000          9\n","Linear(in_features=768, out_      6749.6052       1.4084    5399.9658       0.3361          6\n","Linear(in_features=768, out_     12792.8083       0.9419   10234.4350       0.7545         12\n","Linear(in_features=768, out_      5473.4649       1.4613    4379.0642       0.2478          5\n","Linear(in_features=768, out_     10251.8808       0.8756    8201.6797       0.5785         10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:53, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.234300</td>\n","      <td>0.282096</td>\n","      <td>0.897936</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.157600</td>\n","      <td>0.327229</td>\n","      <td>0.901376</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.155300</td>\n","      <td>0.362788</td>\n","      <td>0.905963</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9060\n","\n","\n","Starting Run 4 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:18, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.520500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.361300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.314800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.351200</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.296100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.314800</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.339000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.322900</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.295900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.317200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      1712.0901       1.0245    1369.8770       0.0127          2\n","Linear(in_features=768, out_      6658.6375       0.9774    5327.1055       0.5071          9\n","Linear(in_features=768, out_      1585.0455       1.0491    1268.2462       0.0000          2\n","Linear(in_features=768, out_     11590.8493       0.9228    9272.8640       1.0000         16\n","Linear(in_features=768, out_      1747.5166       0.8983    1398.1930       0.0162          2\n","Linear(in_features=768, out_            inf       0.8667          inf       0.5000          9\n","Linear(in_features=768, out_      4390.6975       1.2462    3512.8073       0.2804          5\n","Linear(in_features=768, out_            inf       0.9998          inf       0.5000          9\n","Linear(in_features=768, out_      5612.4631       1.3105    4490.2326       0.4025          7\n","Linear(in_features=768, out_            inf       1.0180          inf       0.5000          9\n","Linear(in_features=768, out_      4661.0281       1.5695    3729.1364       0.3074          6\n","Linear(in_features=768, out_            inf       0.9208          inf       0.5000          9\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:57, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.247700</td>\n","      <td>0.278599</td>\n","      <td>0.892202</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.158100</td>\n","      <td>0.310030</td>\n","      <td>0.905963</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.165500</td>\n","      <td>0.349101</td>\n","      <td>0.904817</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9048\n","\n","\n","Starting Run 5 | Warmup Steps = 500\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:18, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.500800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.362900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.312100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.357900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.300500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.318800</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.336600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.314100</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.299100</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.319500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Layer-wise Stats and Rank Assignment:\n","Layer                             Grad Norm     Weight Î”        Score   Norm Score Assigned r\n","Linear(in_features=768, out_      1517.3959       0.9652    1214.1098       0.0318          2\n","Linear(in_features=768, out_      7260.6183       0.9647    5808.6876       0.6486         11\n","Linear(in_features=768, out_      1221.5392       0.9617     977.4237       0.0000          2\n","Linear(in_features=768, out_     10533.0557       0.8934    8426.6233       1.0000         16\n","Linear(in_features=768, out_      2172.8666       0.9509    1738.4835       0.1022          3\n","Linear(in_features=768, out_            inf       0.8391          inf       0.5000          9\n","Linear(in_features=768, out_      4273.4144       1.2757    3418.9866       0.3278          6\n","Linear(in_features=768, out_            inf       1.0035          inf       0.5000          9\n","Linear(in_features=768, out_      5839.9196       1.3805    4672.2118       0.4960          8\n","Linear(in_features=768, out_      9939.0937       0.9689    7951.4688       0.9362         15\n","Linear(in_features=768, out_      4453.1522       1.4809    3562.8179       0.3471          6\n","Linear(in_features=768, out_            inf       0.9453          inf       0.5000          9\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 06:55, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.232800</td>\n","      <td>0.272890</td>\n","      <td>0.892202</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.153200</td>\n","      <td>0.334888</td>\n","      <td>0.903670</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.150800</td>\n","      <td>0.343284</td>\n","      <td>0.911697</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Final Eval Accuracy: 0.9117\n"]}]},{"cell_type":"code","source":["# --- Save all results to CSV ---\n","df = pd.DataFrame(all_results)\n","df.to_csv(\"adaptive_lora_sst2_benchmark.csv\", index=False)\n","\n","# --- Display nicely formatted results ---\n","from tabulate import tabulate\n","print(\"\\nFinal Results Across All Runs:\\n\")\n","print(tabulate(df, headers='keys', tablefmt='pretty'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCBSCiuJB7Rv","executionInfo":{"status":"ok","timestamp":1745774622945,"user_tz":420,"elapsed":76,"user":{"displayName":"Aditya Dawadikar","userId":"04078820344684524161"}},"outputId":"1d2ca309-edaf-46b9-cb30-439e954ee2da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final Results Across All Runs:\n","\n","+---+--------+--------------+----------------+-------------------+---------------------+------------------+\n","|   | Run ID | Warmup Steps | Final Accuracy | Warmup Time (min) | Training Time (min) | Total Time (min) |\n","+---+--------+--------------+----------------+-------------------+---------------------+------------------+\n","| 0 |  0.0   |    500.0     |     0.9094     |       0.31        |        6.87         |       7.18       |\n","| 1 |  1.0   |    500.0     |     0.906      |        0.3        |        6.61         |       6.91       |\n","| 2 |  2.0   |    500.0     |     0.9037     |        0.3        |         6.9         |       7.2        |\n","| 3 |  3.0   |    500.0     |     0.906      |        0.3        |         6.9         |       7.2        |\n","| 4 |  4.0   |    500.0     |     0.9048     |       0.31        |        6.96         |       7.27       |\n","| 5 |  5.0   |    500.0     |     0.9117     |       0.31        |        6.94         |       7.25       |\n","+---+--------+--------------+----------------+-------------------+---------------------+------------------+\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xJG0Ui-DX5DK"},"execution_count":null,"outputs":[]}]}